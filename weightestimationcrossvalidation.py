# -*- coding: utf-8 -*-
"""WeightEstimationCrossValidation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ce4sO39oQYvHtxiWy-VRXF2MvA_mFqjx

# Init

## Import Section
"""

from keras.applications.inception_v3 import InceptionV3
from keras.preprocessing import image
from keras.models import Model
from keras.layers import Dense, GlobalAveragePooling2D
from keras import backend as K
import tensorflow as tf
import keras
from keras.layers import Conv2D,Dense,Flatten,Dropout,MaxPool2D
from keras.optimizers import Adam, Adamax, SGD, RMSprop,Nadam
from keras.models import Model, Sequential
from keras.applications.vgg16 import preprocess_input,VGG16
from keras.preprocessing.image import load_img,img_to_array
import pandas as p
import numpy as np
import os
from keras import metrics
import subprocess
import re
from keras.models import load_model
from scipy import ndimage, misc
from sklearn.model_selection import train_test_split
from keras import regularizers
import skimage
from skimage.io import imread
from skimage import exposure, color
from skimage.transform import resize
from keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
import matplotlib
from skimage.transform import rescale,resize,downscale_local_mean
from PIL import Image
import csv
from glob import glob 
from keras.models import load_model
from datetime import datetime

"""## Parameter Section"""

# Parameters Setting:
now = datetime.now()

epochs = 200
batch_size = 32
shape = (150,150,3)
learning_rate = 0.001
decay = 0.00001
n_fold = 5
resultFileName = 'report' + str(epochs) + 'Epoch' + now.strftime("%d%m%Y_%H_%M_%S") # no extension required
weightFileName = 'weight' + str(epochs) + 'Epoch' + now.strftime("%d%m%Y_%H_%M_%S") # no extension required
modelFileName = 'model' + str(epochs) + 'Epoch' + now.strftime("%d%m%Y_%H_%M_%S") # no extension required
content = './' # base saving and loading folder withouth gDrive

folderName = 'WeightCrossValidation/' + now.strftime("%d%m%Y_%H_%M_%S") + '/'
training_proportion = 0.7
validation_proportion = 0.2
test_proportion = 0.1
save_best_result = True
save_weights_result = True

"""## Folder Configuration"""

# Using gDrive

if os.path.exists(content + folderName):
  savingPath = content + folderName
else:
  savingPath = content + folderName
  os.mkdir(savingPath)

savingWeight = savingPath + 'weights/'
if not os.path.exists(savingWeight):
  os.mkdir(savingWeight)

"""# Model

## Loading Base Model and apply modifiers
"""

base_model = InceptionV3(include_top=False, input_shape=shape)
x = base_model.output
x = GlobalAveragePooling2D()(x) 
x = Dense(1024, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01))(x)
x = Dense(512, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01))(x)
x = Dense(128, activation='relu')(x)
x = Dense(32, activation='relu')(x)
predictions = Dense(1, activation='relu')(x)
model = Model(inputs=base_model.input, outputs=predictions)

"""### Model sumary"""

#model.summary()

"""## Compile Model"""

from keras.optimizers import SGD,Adam,RMSprop
model.compile(optimizer=Adam(lr=learning_rate,decay=decay), loss='mse', metrics=['mae','mse'])

"""### Saving Model"""

# Saving model locally
model.save(savingPath + modelFileName + '.h5')

"""### Loading Weight"""

#model.load_weights(savingPath + weightFileName + '.h5')

"""### Loading Prepared Model"""

#model.load_model(savingPath + modelFileName + 'h5')

"""# Data

##Loading Label
"""

weights=p.read_csv(content + 'weights.csv')
weights = weights['WEIGHT'].values

"""## Random generation of Test set and Execution Set
Explanation: Execution set is the set that contain the data usefull for training and validation
#### Tes set is 10% of the dataset
"""

from random import shuffle
# ID list creation:
id_person_path = sorted(glob(content + 'images/*'))
number_of_people = len(id_person_path)
# Defining dimension
number_of_train_people = int(number_of_people * 0.7) 
number_of_validation_people = number_of_train_people + int(number_of_people * 0.2)
number_of_test_people = number_of_validation_people + int(number_of_people * 0.1)
# Subdivide IDs in test and execution set:
shuffle(id_person_path)
id_person_path_shuffled = id_person_path.copy()
test_set = id_person_path_shuffled[-11:] # prendo gli ultimi 11 indici
cross_set = id_person_path[:-11] # prendo tutti i rimasti per la cross validation
# Test set creation:
img_array_test = []
lab_array_test = []
for id_p in test_set:
  #print(id_p)
  # Obtain ID
  number = int(id_p.split('/')[2])
  # Obtain paths to images
  frames = glob(id_p + '/*')
  # Opening images with skimage
  for image in frames:
    file = imread(image)
    img_array_test.append(file)
    lab_array_test.append(weights[number])

# Convertion to numpy array of test set:
lab_np_array_test = np.array(lab_array_test)
img_np_array_test = np.array(img_array_test)
img_np_array_test = np.repeat(img_np_array_test[:,:,:,np.newaxis], 3, axis=3)
# Number of IDs for Training and Validation set:
number_of_people = len(cross_set)
number_of_train_people = int(number_of_people * (float(n_fold-1)/n_fold)) 
number_of_validation_people = number_of_train_people + int(number_of_people * (float(1)/n_fold))

print(number_of_train_people, number_of_validation_people, number_of_test_people)

# Saving Numpy Test Set
np.save(savingPath + 'img_test_set', img_np_array_test)
np.save(savingPath + 'lab_test_set', lab_np_array_test)


"""### Statistical Analysis of Train data"""

# Media e Deviazione Standard dei pesi
test_weight_std = np.std(lab_array_test)
test_weight_mean = np.mean(lab_array_test)
print('STD of test set: ', test_weight_std)
print('MEAN of test set: ', test_weight_mean)

"""### Saving Statistical Analysis in a text file"""

statSave = open(savingPath + 'StatisticalAnalysis.txt', 'a+')
statSave.write('Test Set Statistical Analysis:')
statSave.write('\n\t STD of test set: {0}; \n\t MEAN of test set: {1}'.format(test_weight_std, test_weight_mean))
statSave.close()

"""## Using Data augmentation with keras"""

# Data augmentation
datagen = ImageDataGenerator(
    featurewise_center=False,
    featurewise_std_normalization=False,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True)

# compute quantities required for featurewise normalization
# (std, mean, and principal components if ZCA whitening is applied)
# Da usare solo se featurewise_center e featurewise_std_normalization sono a True
#datagen.fit(img_np_array_training)

"""# Cross Validation"""

history = {}
for fold in range(0,n_fold):
  print('Fold number: ', fold)
  print('\t Reloading Empty model')
  # Load clear model
  model = load_model(savingPath  + modelFileName + '.h5')

  print('\t Staring Data Creation')
  # Empty Set Creation
  img_array_training = []
  lab_array_training = []
  img_array_validation = []
  lab_array_validation = []

  # Creation of validation and Train Set of IDs:
  low_validation_index = fold * (number_of_validation_people - number_of_train_people)
  hight_validation_index = (fold + 1) * (number_of_validation_people - number_of_train_people)
  validation_set = cross_set[low_validation_index:hight_validation_index]
  validation_set_copy = validation_set.copy()
  train_set = [item for item in cross_set if item not in validation_set]
  
  # Creation of Validation Set
  for id_p in validation_set:
    # Obtain ID
    number = int(id_p.split('/')[2])
    # Obtain paths to images
    frames = glob(id_p + '/*')
    # Opening images with skimage
    for image in frames:
      file = imread(image)
      img_array_validation.append(file)
      lab_array_validation.append(weights[number])

  # Creation of Train Set
  for id_p in train_set:
    # Obtain ID
    number = int(id_p.split('/')[2])
    # Obtain paths to images
    frames = glob(id_p + '/*')
    # Opening images with skimage
    for image in frames:
      file = imread(image)
      img_array_training.append(file)
      lab_array_training.append(weights[number])
  
  # Statistical Analys of Training Set
  train_weight_std = np.std(lab_array_training)
  train_weight_mean = np.mean(lab_array_training)
  print('\t\t STD of train set: ', train_weight_std)
  print('\t\t MEAN of train set: ', train_weight_mean)

  # Statistical Analys of Training Set
  validation_weight_std = np.std(lab_array_validation)
  validation_weight_mean = np.mean(lab_array_validation)
  print('\t\t STD of validation set: ', validation_weight_std)
  print('\t\t MEAN of validation set: ', validation_weight_mean)

  # Conversion to numpy array:
  img_np_array_training = np.array(img_array_training)
  img_np_array_validation = np.array(img_array_validation)
  lab_np_array_training = np.array(lab_array_training)
  lab_np_array_validation = np.array(lab_array_validation)

  # Duplicazione dei canali
  img_np_array_training = np.repeat(img_np_array_training[:,:,:,np.newaxis], 3, axis=3)
  img_np_array_validation = np.repeat(img_np_array_validation[:,:,:,np.newaxis], 3, axis=3)
  
  # Saving Numpy Train Set
  np.save(savingPath + 'img_train_set_' + str(fold), img_np_array_training)
  np.save(savingPath + 'lab_train_set_' + str(fold), lab_np_array_training)
  np.save(savingPath + 'img_validation_set_' + str(fold), img_np_array_validation)
  np.save(savingPath + 'lab_validation_set_' + str(fold), lab_np_array_validation)


  print('\t Data creation Finished')

  statSave = open(savingPath + 'StatisticalAnalysis.txt', 'a+')
  statSave.write('\nFold number: {0} \n\t STD of train set: {1}; \n\t MEAN of train set: {2}; \n\t STD of validation set: {3}; \n\t MEAN of validation set: {4}'.format(fold, train_weight_std, train_weight_mean, validation_weight_std, validation_weight_mean))
  statSave.close()

  # Creazione callbacks di salvataggio
  filepath = "saved-model-{epoch:03d}-{val_mean_absolute_error:.5f}-" + str(fold) + ".h5" # dynamic name
  checkpoint = keras.callbacks.ModelCheckpoint(filepath= savingWeight + filepath, monitor='val_mean_absolute_error', verbose=0, save_best_only=save_best_result, save_weights_only=save_weights_result, mode='min', period=1)
  
  print('\t Starting Training')
  # Starting Training
  history[fold] = model.fit_generator(datagen.flow(img_np_array_training, lab_np_array_training, batch_size=batch_size),
                    steps_per_epoch=len(img_np_array_training) / batch_size, epochs=epochs, validation_data=(img_np_array_validation,lab_np_array_validation), callbacks=[checkpoint])

"""# Results

## Mean of best model results
"""

# Taking best model results:
savingPath = content + 'WeightCrossValidation/'
#bestModelFolder = savingPath + 'CrossValidation/'
# Selecting the minimum mae from saved weights:
# Initialize a minimum reference
minimum_value = {}
minimum_fileName = {}
# init of dictionary of the minimum
for key in range(0,n_fold):
  minimum_value[key] = 100.000
for file in os.listdir(savingWeight):
    if file.endswith(".h5"):
      name = file.replace('.h5','')
      value = float(name.split('-')[3])
      key = int(name.split('-')[4])
      if value < minimum_value[key]:
        minimum_value[key] = value
        minimum_fileName[key] = file
print(minimum_fileName)
print(minimum_value)
test_mae = {}
for key in minimum_fileName.keys():
  model.load_weights(savingWeight + minimum_fileName[key])
  #print(model.metrics_names)
  loss, mae, acc, mse = model.evaluate(img_np_array_test, lab_np_array_test, verbose=0)
  test_mae[key] = mae
  print('Testing set Mean Abs Error: {:5.2f} Kg'.format(mae),'\t Testing Mean Square Error: {:5.2f} kg'.format(mse))

  pred = model.predict(img_np_array_test)
  
  predictions = model.predict(img_np_array_test).flatten()
  plt.figure(str(key))
  plt.scatter(lab_np_array_test, predictions)
  plt.xlabel('True Values [Kg]')
  plt.ylabel('Predictions [Kg]')
  plt.axis('equal')
  plt.axis('square')
  plt.xlim([0,plt.xlim()[1]])
  plt.ylim([0,plt.ylim()[1]])
  _ = plt.plot([-100, 150], [-100, 150])
  plt.savefig(savingPath + 'pred_inc-' + str(key) + '.pdf')
  percentage_error_list = []
  for k, j in np.ndenumerate(pred):
    percentage_of_error = abs(((j - lab_np_array_test[k[0]])/(lab_np_array_test[k[0]])) * 100 )
    percentage_error_list.append(percentage_of_error)
  percentage_error_array = np.array(percentage_error_list)
  np.save(savingPath + 'test_error_array' + str(key), percentage_error_array)
  print('For fold {0} and value error {1} the mean percentage error is: {2} with standard deviation of {3}'.format(key, minimum_value[key], np.mean(percentage_error_array), np.std(percentage_error_array)))

summ_validation = 0
summ_test = 0
for key in minimum_value.keys():
  summ_validation += minimum_value[key]
  summ_test += test_mae[key]
mean_validation = summ_validation/len(minimum_value.keys())
mean_test = summ_test/len(test_mae.keys())
print('The valuation mean of the mae for {0} folds is: {1}'.format(n_fold,mean_validation))
print('The test mean of the mae for {0} is: {1}'.format(n_fold, mean_test))
